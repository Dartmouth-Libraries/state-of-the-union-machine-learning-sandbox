{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify the party affiliation of the president"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import the data into dataframe \"\"\"\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)  # We want to see the whole dataframe\n",
    "\n",
    "dataset_folder = Path('../state-of-the-union-dataset')\n",
    "full_text_folder = dataset_folder / 'txt'\n",
    "meta_folder = dataset_folder / 'meta'\n",
    "\n",
    "speeches = []\n",
    "presidents = []\n",
    "years = []\n",
    "\n",
    "for file in full_text_folder.glob('*.txt'):\n",
    "    speeches.append(file.read_text())\n",
    "    president, year = file.stem.split('_')\n",
    "    presidents.append(president)\n",
    "    years.append(int(year))\n",
    "\n",
    "df = pd.DataFrame(index=years, data={'President': presidents, 'Text': speeches}).sort_index()\n",
    "\n",
    "# Read metadata\n",
    "presidents = pd.read_csv(meta_folder / 'presidents.csv', converters={\"Party\": literal_eval})\n",
    "presidents['First Year'] = presidents['Term Start'].str.extract(r',\\s([0-9]{4})').astype(\"int\")\n",
    "presidents['Last Year'] = presidents['Term End'].str.extract(r',\\s([0-9]{4})').astype(\"float\")\n",
    "speeches_meta = pd.read_csv(meta_folder / 'speeches-meta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_party_case(year, president):\n",
    "    \"\"\" Manually handle some cases where the party affiliation is ambiguous \"\"\"\n",
    "    party = None\n",
    "    if president['Last Name'] in ['Adams', 'Tyler', 'Johnson']:\n",
    "        party = president['Party'][0]\n",
    "    else:\n",
    "        raise NotImplementedError(\"[handle_special_party_case] Unhandled special case!\")   \n",
    "    return party\n",
    "\n",
    "def add_meta(row):\n",
    "    \"\"\" Adds meta information to a row in the dataframe \"\"\"\n",
    "    year = row.name\n",
    "    last_name = row['President']\n",
    "    president = presidents[(presidents['First Year'] <= year) & (presidents['Last Year'] > year)].squeeze()\n",
    "    first_name = president['First Name(s)']\n",
    "    row['First Name'] = first_name\n",
    "    party = president['Party']\n",
    "    if len(party) > 1:\n",
    "        party = handle_special_party_case(year, president)\n",
    "    else:\n",
    "        party = party[0]\n",
    "        \n",
    "    row['Party'] = party\n",
    "\n",
    "    return row\n",
    "\n",
    "df = df.apply(add_meta, axis='columns')\n",
    "df.insert(0, 'First Name', df.pop('First Name'))\n",
    "df.insert(1, 'Last Name', df.pop('President'))\n",
    "df.insert(2, 'Party', df.pop('Party'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stopwords_ = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand some contractions\n",
    "df['Expanded Text'] = df[\"Text\"].str.replace(\"'ll\", \" will\").str.replace(\"'ve\", \" have\").str.replace(\"'re\", \"are\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokens'] = df['Expanded Text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "df['Cleaned Tokens'] = df['Tokens'].apply(lambda tokens: [w.lower() for w in tokens if not w.lower() in stopwords_.union(set([c for c in string.punctuation] + ['--', \"''\", '``']))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "df['Tagged Tokens'] = df['Cleaned Tokens'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tagged_tokens):\n",
    "    lemmatized_tokens = []\n",
    "    for token, tag in tagged_tokens:\n",
    "        if tag in ['JJ', 'JJR', 'JJS']:\n",
    "            pos = 'a'\n",
    "        elif tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            pos = 'n'\n",
    "        elif tag in ['RB', 'RBR', 'RBS', 'WRB']:\n",
    "            pos = 'r'\n",
    "        elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            lemmatized_tokens.append(token)\n",
    "            continue\n",
    "        \n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token, pos))\n",
    "    return lemmatized_tokens\n",
    "    \n",
    "\n",
    "df['Lemmatized'] = df['Tagged Tokens'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from gensim import corpora, models\n",
    "from gensim.matutils import corpus2dense\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class TopicModeler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" A topic modeler with an sklearn-compatible interface \"\"\"\n",
    "    def __init__(self, num_topics=10):\n",
    "        self.num_topics = num_topics\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        X = X.values\n",
    "        self.dictionary = corpora.Dictionary(X)\n",
    "        corpus = [self.dictionary.doc2bow(text) for text in X]\n",
    "        self.tfidf = models.TfidfModel(corpus)\n",
    "        corpus_tfidf = self.tfidf[corpus]\n",
    "        self.lsi_model = models.LsiModel(corpus_tfidf, num_topics=self.num_topics)\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.values\n",
    "        corpus = [self.dictionary.doc2bow(text) for text in X]\n",
    "        \n",
    "        X_topics = []\n",
    "        for topics in self.lsi_model[self.tfidf[corpus]]:\n",
    "            t = [topic for _, topic in topics]\n",
    "            X_topics.append(t)\n",
    "        return np.array(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare the Pipeline \"\"\"\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "vectorizer = TfidfVectorizer(input='content', \n",
    "                             lowercase=False, \n",
    "                             preprocessor=lambda x: x,   # We did the preprocessing ourselves, so just pass everything through\n",
    "                             tokenizer=lambda x: x,      # We also did the tokenization ourselves\n",
    "                             ngram_range=(2, 2),         # Use bigrams\n",
    "                             max_features=1000)          # Limit the vocabulary to 1000 words\n",
    "\n",
    "feature_extractor = ColumnTransformer([\n",
    "    ('tfidf', vectorizer, 'Lemmatized'),\n",
    "    ('topics', TopicModeler(), 'Lemmatized')\n",
    "])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('feature_extraction', feature_extractor), \n",
    "    ('classifier', SVC())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Holdout validate pipeline \"\"\"\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df_train, df_test = train_test_split(df[(df['Party'] == 'Democratic') | (df['Party'] == 'Republican')], test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Grid search hyperparamters \"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"feature_extraction__topics__num_topics\": [5, 10, 15, 20, 25],\n",
    "    'classifier__C': [100, 1000, 10e3, 10e4], \n",
    "    'classifier__gamma': [0.001, 0.01, 0.1], \n",
    "    'classifier__kernel': ['rbf'],\n",
    "}\n",
    "\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=8, verbose=3)\n",
    "search.fit(df_train, df_train['Party'])\n",
    "print(f\"Best parameter (CV score={search.best_score_:0.3f}):\")\n",
    "print(search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(df_test['Party'], search.predict(df_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
